{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statistical-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data.txt', 'r') as in_file:\n",
    "    stripped = (line.strip() for line in in_file)\n",
    "    lines = (line.split(\",\") for line in stripped if line)\n",
    "    with open('log.csv', 'w', newline='') as out_file:\n",
    "        writer = csv.writer(out_file)\n",
    "        writer.writerow(('duration', 'protocol_type', 'service', 'src_bytes', 'dst_bytes', 'flag', 'land', 'wrong_fragment','urgent','hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations','num_shells', 'num_access_files', 'num_outbound_cmds', 'is_hot_login', 'is_guest_login','count', 'serror_rate', 'rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_count', 'srv_serror_rate', 'srv_rerror_rate', 'srv_diff_host_rate','unknown_data1','unknown_data2','unknown_data3','unknown_data4','unknown_data5','unknown_data6','unknown_data7','unknown_data8','unknown_data9','unknown_data10','results'))\n",
    "        writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hispanic-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow\n",
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "productive-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('log1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effective-medicare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494021, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rental-google",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>flag</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>unknown_data2</th>\n",
       "      <th>unknown_data3</th>\n",
       "      <th>unknown_data4</th>\n",
       "      <th>unknown_data5</th>\n",
       "      <th>unknown_data6</th>\n",
       "      <th>unknown_data7</th>\n",
       "      <th>unknown_data8</th>\n",
       "      <th>unknown_data9</th>\n",
       "      <th>unknown_data10</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>54540</td>\n",
       "      <td>8314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>54540</td>\n",
       "      <td>8314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>54540</td>\n",
       "      <td>8314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>54540</td>\n",
       "      <td>8314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>54540</td>\n",
       "      <td>8314</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type service src_bytes  dst_bytes  flag  land  \\\n",
       "0         0           tcp    http        SF      54540  8314     0   \n",
       "1         0           tcp    http        SF      54540  8314     0   \n",
       "2         0           tcp    http        SF      54540  8314     0   \n",
       "3         0           tcp    http        SF      54540  8314     0   \n",
       "4         0           tcp    http        SF      54540  8314     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  unknown_data2  unknown_data3  \\\n",
       "0               0       0    2  ...              1            1.0   \n",
       "1               0       0    2  ...              2            1.0   \n",
       "2               0       0    2  ...              3            1.0   \n",
       "3               0       0    2  ...              4            1.0   \n",
       "4               0       0    2  ...              5            1.0   \n",
       "\n",
       "   unknown_data4  unknown_data5  unknown_data6  unknown_data7  unknown_data8  \\\n",
       "0            0.0           1.00            0.0            0.0            0.0   \n",
       "1            0.0           0.50            0.0            0.0            0.0   \n",
       "2            0.0           0.33            0.0            0.0            0.0   \n",
       "3            0.0           0.25            0.0            0.0            0.0   \n",
       "4            0.0           0.20            0.0            0.0            0.0   \n",
       "\n",
       "   unknown_data9  unknown_data10  results  \n",
       "0            0.0             0.0       21  \n",
       "1            0.0             0.0       21  \n",
       "2            0.0             0.0       21  \n",
       "3            0.0             0.0       21  \n",
       "4            0.0             0.0       21  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "economic-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 395217 samples for training and 98804 for validation\n"
     ]
    }
   ],
   "source": [
    "val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n",
    "train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "print(\n",
    "    \"Using %d samples for training and %d for validation\"\n",
    "    % (len(train_dataframe), len(val_dataframe))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "satisfied-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"results\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pediatric-yesterday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'duration': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'protocol_type': <tf.Tensor: shape=(), dtype=string, numpy=b'icmp'>, 'service': <tf.Tensor: shape=(), dtype=string, numpy=b'ecr_i'>, 'src_bytes': <tf.Tensor: shape=(), dtype=string, numpy=b'SF'>, 'dst_bytes': <tf.Tensor: shape=(), dtype=int64, numpy=520>, 'flag': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'land': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'wrong_fragment': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'urgent': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'hot': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_failed_logins': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'logged_in': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_compromised': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'root_shell': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'su_attempted': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_root': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_file_creations': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_shells': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_access_files': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_outbound_cmds': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'is_hot_login': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'is_guest_login': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'count': <tf.Tensor: shape=(), dtype=int64, numpy=511>, 'serror_rate': <tf.Tensor: shape=(), dtype=int64, numpy=511>, 'rerror_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'same_srv_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'diff_srv_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'srv_count': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'srv_serror_rate': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>, 'srv_rerror_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'srv_diff_host_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data1': <tf.Tensor: shape=(), dtype=int64, numpy=255>, 'unknown_data2': <tf.Tensor: shape=(), dtype=int64, numpy=255>, 'unknown_data3': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>, 'unknown_data4': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data5': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>, 'unknown_data6': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data7': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data8': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data9': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data10': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>}\n",
      "Target: tf.Tensor(6, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "english-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "violent-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_string_categorical_feature(feature, name, dataset):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    index = StringLookup()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = index(feature)\n",
    "\n",
    "    # Create a CategoryEncoding for our integer indices\n",
    "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a dataset of indices\n",
    "    feature_ds = feature_ds.map(index)\n",
    "\n",
    "    # Learn the space of possible indices\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices\n",
    "    encoded_feature = encoder(encoded_feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_integer_categorical_feature(feature, name, dataset):\n",
    "    # Create a CategoryEncoding for our integer indices\n",
    "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the space of possible indices\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices\n",
    "    encoded_feature = encoder(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "existing-strengthening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "roman-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = keras.Input(shape=(1,), name=\"duration\")\n",
    "protocol_type = keras.Input(shape=(1,), name=\"protocol_type\", dtype=\"string\")\n",
    "service = keras.Input(shape=(1,), name=\"service\", dtype=\"string\")\n",
    "src_bytes = keras.Input(shape=(1,), name=\"src_bytes\", dtype=\"string\")\n",
    "dst_bytes = keras.Input(shape=(1,), name=\"dst_bytes\")\n",
    "flag = keras.Input(shape=(1,), name=\"flag\")\n",
    "land = keras.Input(shape=(1,), name=\"land\")\n",
    "# wrong_fragment = keras.Input(shape=(1,), name=\"wrong_fragment\")\n",
    "# urgent = keras.Input(shape=(1,), name=\"urgent\")\n",
    "# hot = keras.Input(shape=(1,), name=\"hot\")\n",
    "# num_failed_logins = keras.Input(shape=(1,), name=\"num_failed_logins\")\n",
    "# logged_in = keras.Input(shape=(1,), name=\"logged_in\")\n",
    "# num_compromised = keras.Input(shape=(1,), name=\"num_compromised\")\n",
    "# root_shell = keras.Input(shape=(1,), name=\"root_shell\")\n",
    "# su_attempted = keras.Input(shape=(1,), name=\"su_attempted\")\n",
    "# num_root = keras.Input(shape=(1,), name=\"num_root\")\n",
    "# num_file_creations = keras.Input(shape=(1,), name=\"num_file_creations\")\n",
    "# num_shells = keras.Input(shape=(1,), name=\"num_shells\")\n",
    "# num_access_files = keras.Input(shape=(1,), name=\"num_access_files\")\n",
    "# num_outbound_cmds = keras.Input(shape=(1,), name=\"num_outbound_cmds\")\n",
    "# is_hot_login = keras.Input(shape=(1,), name=\"is_hot_login\")\n",
    "# is_guest_login = keras.Input(shape=(1,), name=\"is_guest_login\")\n",
    "# count = keras.Input(shape=(1,), name=\"count\")\n",
    "# serror_rate = keras.Input(shape=(1,), name=\"serror_rate\")\n",
    "# rerror_rate = keras.Input(shape=(1,), name=\"rerror_rate\")\n",
    "# same_srv_rate = keras.Input(shape=(1,), name=\"same_srv_rate\")\n",
    "# diff_srv_rate = keras.Input(shape=(1,), name=\"diff_srv_rate\")\n",
    "# srv_count = keras.Input(shape=(1,), name=\"srv_count\")\n",
    "# srv_serror_rate = keras.Input(shape=(1,), name=\"srv_serror_rate\")\n",
    "# srv_rerror_rate = keras.Input(shape=(1,), name=\"srv_rerror_rate\")\n",
    "# srv_diff_host_rate = keras.Input(shape=(1,), name=\"srv_diff_host_rate\")\n",
    "# unknown_data1 = keras.Input(shape=(1,), name=\"unknown_data1\")\n",
    "# unknown_data2 = keras.Input(shape=(1,), name=\"unknown_data2\")\n",
    "# unknown_data3 = keras.Input(shape=(1,), name=\"unknown_data3\")\n",
    "# unknown_data4 = keras.Input(shape=(1,), name=\"unknown_data4\")\n",
    "# unknown_data5 = keras.Input(shape=(1,), name=\"unknown_data5\")\n",
    "# unknown_data6 = keras.Input(shape=(1,), name=\"unknown_data6\")\n",
    "# unknown_data7 = keras.Input(shape=(1,), name=\"unknown_data7\")\n",
    "# unknown_data8 = keras.Input(shape=(1,), name=\"unknown_data8\")\n",
    "# unknown_data9 = keras.Input(shape=(1,), name=\"unknown_data9\")\n",
    "# unknown_data10 = keras.Input(shape=(1,), name=\"unknown_data10\")\n",
    "\n",
    "\n",
    "all_inputs = [duration,\n",
    "              protocol_type, \n",
    "              service, \n",
    "              src_bytes, \n",
    "              dst_bytes, \n",
    "              flag, \n",
    "              land, \n",
    "#               wrong_fragment,\n",
    "#               urgent,\n",
    "#               hot, \n",
    "#               num_failed_logins, \n",
    "#               logged_in, \n",
    "#               num_compromised, \n",
    "#               root_shell, \n",
    "#               su_attempted, \n",
    "#               num_root, \n",
    "#               num_file_creations,\n",
    "#               num_shells, \n",
    "#               num_access_files, \n",
    "#               num_outbound_cmds, \n",
    "#               is_hot_login, \n",
    "#               is_guest_login,\n",
    "#               count, \n",
    "#               serror_rate, \n",
    "#               rerror_rate, \n",
    "#               same_srv_rate, \n",
    "#               diff_srv_rate, \n",
    "#               srv_count, \n",
    "#               srv_serror_rate, \n",
    "#               srv_rerror_rate, \n",
    "#               srv_diff_host_rate,\n",
    "#               unknown_data1,\n",
    "#               unknown_data2,\n",
    "#               unknown_data3,\n",
    "#               unknown_data4,\n",
    "#               unknown_data5,\n",
    "#               unknown_data6,\n",
    "#               unknown_data7,\n",
    "#               unknown_data8,\n",
    "#               unknown_data9,\n",
    "#               unknown_data10\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "weighted-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_encoded = encode_numerical_feature(duration, \"duration\", train_ds)\n",
    "\n",
    "protocol_type_encoded = encode_string_categorical_feature(protocol_type,\"protocol_type\", train_ds)\n",
    "service_encoded = encode_string_categorical_feature(service, \"service\", train_ds)\n",
    "src_bytes_encoded = encode_string_categorical_feature(src_bytes,\"src_bytes\", train_ds)\n",
    "\n",
    "\n",
    "dst_bytes_encoded = encode_numerical_feature(dst_bytes,\"dst_bytes\", train_ds)\n",
    "flag_encoded = encode_numerical_feature(flag, \"flag\", train_ds)\n",
    "land_encoded = encode_numerical_feature(land, \"land\", train_ds)\n",
    "# wrong_fragment_encoded = encode_numerical_feature(wrong_fragment, \"wrong_fragment\", train_ds)\n",
    "# urgent_encoded = encode_numerical_feature(urgent, \"urgent\", train_ds)\n",
    "# hot_encoded = encode_numerical_feature(hot, \"hot\", train_ds)\n",
    "# num_failed_logins_encoded = encode_numerical_feature(num_failed_logins, \"num_failed_logins\", train_ds)\n",
    "# logged_in_encoded = encode_numerical_feature(logged_in, \"logged_in\", train_ds)\n",
    "# num_compromised_encoded = encode_numerical_feature(num_compromised, \"num_compromised\", train_ds)\n",
    "# root_shell_encoded = encode_numerical_feature(root_shell, \"root_shell\", train_ds)\n",
    "# su_attempted_encoded = encode_numerical_feature(su_attempted, \"su_attempted\", train_ds)\n",
    "# num_root_encoded = encode_numerical_feature(num_root, \"num_root\", train_ds)\n",
    "# num_file_creations_encoded = encode_numerical_feature(num_file_creations, \"num_file_creations\", train_ds)\n",
    "# num_shells_encoded = encode_numerical_feature(num_shells, \"num_shells\", train_ds)\n",
    "# num_access_files_encoded = encode_numerical_feature(num_access_files, \"num_access_files\", train_ds)\n",
    "# num_outbound_cmds_encoded = encode_numerical_feature(num_outbound_cmds, \"num_outbound_cmds\", train_ds)\n",
    "# is_hot_login_encoded = encode_numerical_feature(is_hot_login, \"is_hot_login\", train_ds)\n",
    "# is_guest_login_encoded = encode_numerical_feature(is_guest_login, \"is_guest_login\", train_ds)\n",
    "# count_encoded = encode_numerical_feature(count, \"count\", train_ds)\n",
    "# serror_rate_encoded = encode_numerical_feature(serror_rate, \"serror_rate\", train_ds)\n",
    "# rerror_rate_encoded = encode_numerical_feature(rerror_rate, \"rerror_rate\", train_ds)\n",
    "# same_srv_rate_encoded = encode_numerical_feature(same_srv_rate, \"same_srv_rate\", train_ds)\n",
    "# diff_srv_rate_encoded = encode_numerical_feature(diff_srv_rate, \"diff_srv_rate\", train_ds)\n",
    "# srv_count_encoded = encode_numerical_feature(srv_count, \"srv_count\", train_ds)\n",
    "# srv_serror_rate_encoded = encode_numerical_feature(srv_serror_rate, \"srv_serror_rate\", train_ds)\n",
    "# srv_rerror_rate_encoded = encode_numerical_feature(srv_rerror_rate, \"srv_rerror_rate\", train_ds)\n",
    "# srv_diff_host_rate_encoded = encode_numerical_feature(srv_diff_host_rate, \"srv_diff_host_rate\", train_ds)\n",
    "# unknown_data1_encoded = encode_numerical_feature(unknown_data1, \"unknown_data1\", train_ds)\n",
    "# unknown_data2_encoded = encode_numerical_feature(unknown_data2, \"unknown_data2\", train_ds)\n",
    "# unknown_data3_encoded = encode_numerical_feature(unknown_data3, \"unknown_data3\", train_ds)\n",
    "# unknown_data4_encoded = encode_numerical_feature(unknown_data4, \"unknown_data4\", train_ds)\n",
    "# unknown_data5_encoded = encode_numerical_feature(unknown_data5, \"unknown_data5\", train_ds)\n",
    "# unknown_data6_encoded = encode_numerical_feature(unknown_data6, \"unknown_data6\", train_ds)\n",
    "# unknown_data7_encoded = encode_numerical_feature(unknown_data7, \"unknown_data7\", train_ds)\n",
    "# unknown_data8_encoded = encode_numerical_feature(unknown_data8, \"unknown_data8\", train_ds)\n",
    "# unknown_data9_encoded = encode_numerical_feature(unknown_data9, \"unknown_data9\", train_ds)\n",
    "# unknown_data10_encoded = encode_numerical_feature(unknown_data10, \"unknown_data10\", train_ds)\n",
    "\n",
    "\n",
    "\n",
    "all_features = layers.concatenate(\n",
    "    [\n",
    "        duration_encoded,\n",
    "        protocol_type_encoded, \n",
    "        service_encoded, \n",
    "        src_bytes_encoded, \n",
    "        dst_bytes_encoded,\n",
    "        flag_encoded, \n",
    "        land_encoded, \n",
    "#         wrong_fragment_encoded,\n",
    "#         urgent_encoded,\n",
    "#         hot_encoded, \n",
    "#         num_failed_logins_encoded, \n",
    "#         logged_in_encoded, \n",
    "#         num_compromised_encoded, \n",
    "#         root_shell_encoded, \n",
    "#         su_attempted_encoded, \n",
    "#         num_root_encoded, \n",
    "#         num_file_creations_encoded,\n",
    "#         num_shells_encoded, \n",
    "#         num_access_files_encoded, \n",
    "#         num_outbound_cmds_encoded, \n",
    "#         is_hot_login_encoded, \n",
    "#         is_guest_login_encoded,\n",
    "#         count_encoded, \n",
    "#         serror_rate_encoded, \n",
    "#         rerror_rate_encoded, \n",
    "#         same_srv_rate_encoded, \n",
    "#         diff_srv_rate_encoded, \n",
    "#         srv_count_encoded, \n",
    "#         srv_serror_rate_encoded, \n",
    "#         srv_rerror_rate_encoded, \n",
    "#         srv_diff_host_rate_encoded,\n",
    "#         unknown_data1_encoded,\n",
    "#         unknown_data2_encoded,\n",
    "#         unknown_data3_encoded,\n",
    "#         unknown_data4_encoded,\n",
    "#         unknown_data5_encoded,\n",
    "#         unknown_data6_encoded,\n",
    "#         unknown_data7_encoded,\n",
    "#         unknown_data8_encoded,\n",
    "#         unknown_data9_encoded,\n",
    "#         unknown_data10_encoded,\n",
    "\n",
    "    ]\n",
    ")\n",
    "                                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "altered-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12351/12351 [==============================] - 96s 5ms/step - loss: -574866.4110 - accuracy: 0.2166 - val_loss: -7131725.5000 - val_accuracy: 0.2165\n",
      "Epoch 2/5\n",
      "12351/12351 [==============================] - 65s 4ms/step - loss: -13306054.6932 - accuracy: 0.2176 - val_loss: -42814340.0000 - val_accuracy: 0.2165\n",
      "Epoch 3/5\n",
      "12351/12351 [==============================] - 70s 4ms/step - loss: -59331822.5019 - accuracy: 0.2178 - val_loss: -126488632.0000 - val_accuracy: 0.2165\n",
      "Epoch 4/5\n",
      "12351/12351 [==============================] - 69s 4ms/step - loss: -157902662.4527 - accuracy: 0.2177 - val_loss: -277085664.0000 - val_accuracy: 0.2165\n",
      "Epoch 5/5\n",
      "12351/12351 [==============================] - 72s 4ms/step - loss: -327606503.9093 - accuracy: 0.2171 - val_loss: -513063424.0000 - val_accuracy: 0.2165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2200125f198>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = layers.Dense(7, activation=\"relu\")(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x1 = layers.Dense(32, activation=\"relu\")(x)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x1)\n",
    "\n",
    "model = keras.Model(all_inputs, output)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adaptive-lemon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"dot\" with args ['-Tps', 'C:\\\\Users\\\\hasith\\\\AppData\\\\Local\\\\Temp\\\\tmpi_ps806x'] returned code: 1\n",
      "\n",
      "stdout, stderr:\n",
      " b''\n",
      "b'Format: \"ps\" not recognized. Use one of:\\r\\n'\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-fe1cdfed02fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"LR\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mf:\\zzzz_projects\\kasun nn web design\\env\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[0;32m    329\u001b[0m       \u001b[0mrankdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrankdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m       \u001b[0mexpand_nested\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand_nested\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m       dpi=dpi)\n\u001b[0m\u001b[0;32m    332\u001b[0m   \u001b[0mto_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\zzzz_projects\\kasun nn web design\\env\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     message = (\n\u001b[0;32m    107\u001b[0m         \u001b[1;34m'Failed to import pydot. You must `pip install pydot` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\zzzz_projects\\kasun nn web design\\env\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mcheck_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Attempt to create an image of a blank graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvocationException\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\zzzz_projects\\kasun nn web design\\env\\lib\\site-packages\\pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format, encoding)\u001b[0m\n\u001b[0;32m   1943\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1945\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1947\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstdout_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "coordinate-investigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "This particular patient had a 100.0 percent probability of having a heart disease, as evaluated by our model.\n"
     ]
    }
   ],
   "source": [
    "sample = {\n",
    "    \"duration\"           : 10,\n",
    "    \"protocol_type\"      : 'icmp', \n",
    "    \"service\"            : 'ecr_i', \n",
    "    \"src_bytes\"          : 'SF', \n",
    "    \"dst_bytes\"          : 11, \n",
    "    \"flag\"               : 0, \n",
    "    \"land\"               : 0, \n",
    "#     \"age\": 60,\n",
    "#     \"sex\": 1,\n",
    "#     \"cp\": 1,\n",
    "#     \"trestbps\": 145,\n",
    "#     \"chol\": 233,\n",
    "#     \"fbs\": 1,\n",
    "#     \"restecg\": 2,\n",
    "#     \"thalach\": 150,\n",
    "#     \"exang\": 0,\n",
    "#     \"oldpeak\": 2.3,\n",
    "#     \"slope\": 3,\n",
    "#     \"ca\": 0,\n",
    "#     \"thal\": \"fixed\",\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "predictions = model.predict(input_dict)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "print(\n",
    "    \"This particular patient had a %.1f percent probability \"\n",
    "    \"of having a heart disease, as evaluated by our model.\" % (100 * predictions[0][0],)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-attachment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
