{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statistical-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# with open('data.txt', 'r') as in_file:\n",
    "#     stripped = (line.strip() for line in in_file)\n",
    "#     lines = (line.split(\",\") for line in stripped if line)\n",
    "#     with open('log.csv', 'w', newline='') as out_file:\n",
    "#         writer = csv.writer(out_file)\n",
    "#         writer.writerow(('duration', 'protocol_type', 'service', 'src_bytes', 'dst_bytes', 'flag', 'land', 'wrong_fragment','urgent','hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations','num_shells', 'num_access_files', 'num_outbound_cmds', 'is_hot_login', 'is_guest_login','count', 'serror_rate', 'rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_count', 'srv_serror_rate', 'srv_rerror_rate', 'srv_diff_host_rate','unknown_data1','unknown_data2','unknown_data3','unknown_data4','unknown_data5','unknown_data6','unknown_data7','unknown_data8','unknown_data9','unknown_data10','results'))\n",
    "#         writer.writerows(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hispanic-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow\n",
    "# !pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "productive-pleasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('log2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effective-medicare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494021, 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rental-google",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>protocol_type</th>\n",
       "      <th>service</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>flag</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>...</th>\n",
       "      <th>unknown_data2</th>\n",
       "      <th>unknown_data3</th>\n",
       "      <th>unknown_data4</th>\n",
       "      <th>unknown_data5</th>\n",
       "      <th>unknown_data6</th>\n",
       "      <th>unknown_data7</th>\n",
       "      <th>unknown_data8</th>\n",
       "      <th>unknown_data9</th>\n",
       "      <th>unknown_data10</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>181</td>\n",
       "      <td>5450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>239</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>235</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>219</td>\n",
       "      <td>1337</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>http</td>\n",
       "      <td>SF</td>\n",
       "      <td>217</td>\n",
       "      <td>2032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>49</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration protocol_type service src_bytes  dst_bytes  flag  land  \\\n",
       "0         0           tcp    http        SF        181  5450     0   \n",
       "1         0           tcp    http        SF        239   486     0   \n",
       "2         0           tcp    http        SF        235  1337     0   \n",
       "3         0           tcp    http        SF        219  1337     0   \n",
       "4         0           tcp    http        SF        217  2032     0   \n",
       "\n",
       "   wrong_fragment  urgent  hot  ...  unknown_data2  unknown_data3  \\\n",
       "0               0       0    0  ...              9            1.0   \n",
       "1               0       0    0  ...             19            1.0   \n",
       "2               0       0    0  ...             29            1.0   \n",
       "3               0       0    0  ...             39            1.0   \n",
       "4               0       0    0  ...             49            1.0   \n",
       "\n",
       "   unknown_data4  unknown_data5  unknown_data6  unknown_data7  unknown_data8  \\\n",
       "0            0.0           0.11            0.0            0.0            0.0   \n",
       "1            0.0           0.05            0.0            0.0            0.0   \n",
       "2            0.0           0.03            0.0            0.0            0.0   \n",
       "3            0.0           0.03            0.0            0.0            0.0   \n",
       "4            0.0           0.02            0.0            0.0            0.0   \n",
       "\n",
       "   unknown_data9  unknown_data10  results  \n",
       "0            0.0             0.0        0  \n",
       "1            0.0             0.0        0  \n",
       "2            0.0             0.0        0  \n",
       "3            0.0             0.0        0  \n",
       "4            0.0             0.0        0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "economic-contamination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 395217 samples for training and 98804 for validation\n"
     ]
    }
   ],
   "source": [
    "val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n",
    "train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "print(\n",
    "    \"Using %d samples for training and %d for validation\"\n",
    "    % (len(train_dataframe), len(val_dataframe))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "satisfied-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"results\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "val_ds = dataframe_to_dataset(val_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pediatric-yesterday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'duration': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'protocol_type': <tf.Tensor: shape=(), dtype=string, numpy=b'tcp'>, 'service': <tf.Tensor: shape=(), dtype=string, numpy=b'private'>, 'src_bytes': <tf.Tensor: shape=(), dtype=string, numpy=b'S0'>, 'dst_bytes': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'flag': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'land': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'wrong_fragment': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'urgent': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'hot': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_failed_logins': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'logged_in': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_compromised': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'root_shell': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'su_attempted': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_root': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_file_creations': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_shells': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_access_files': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'num_outbound_cmds': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'is_hot_login': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'is_guest_login': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'count': <tf.Tensor: shape=(), dtype=int64, numpy=130>, 'serror_rate': <tf.Tensor: shape=(), dtype=int64, numpy=19>, 'rerror_rate': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>, 'same_srv_rate': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>, 'diff_srv_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'srv_count': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'srv_serror_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.15>, 'srv_rerror_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.06>, 'srv_diff_host_rate': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data1': <tf.Tensor: shape=(), dtype=int64, numpy=255>, 'unknown_data2': <tf.Tensor: shape=(), dtype=int64, numpy=19>, 'unknown_data3': <tf.Tensor: shape=(), dtype=float64, numpy=0.07>, 'unknown_data4': <tf.Tensor: shape=(), dtype=float64, numpy=0.07>, 'unknown_data5': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data6': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data7': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>, 'unknown_data8': <tf.Tensor: shape=(), dtype=float64, numpy=1.0>, 'unknown_data9': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>, 'unknown_data10': <tf.Tensor: shape=(), dtype=float64, numpy=0.0>}\n",
      "Target: tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "english-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "violent-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "\n",
    "\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_string_categorical_feature(feature, name, dataset):\n",
    "    # Create a StringLookup layer which will turn strings into integer indices\n",
    "    index = StringLookup()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    index.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = index(feature)\n",
    "\n",
    "    # Create a CategoryEncoding for our integer indices\n",
    "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a dataset of indices\n",
    "    feature_ds = feature_ds.map(index)\n",
    "\n",
    "    # Learn the space of possible indices\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices\n",
    "    encoded_feature = encoder(encoded_feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_integer_categorical_feature(feature, name, dataset):\n",
    "    # Create a CategoryEncoding for our integer indices\n",
    "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the space of possible indices\n",
    "    encoder.adapt(feature_ds)\n",
    "\n",
    "    # Apply one-hot encoding to our indices\n",
    "    encoded_feature = encoder(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "existing-strengthening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "roman-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = keras.Input(shape=(1,), name=\"duration\")\n",
    "protocol_type = keras.Input(shape=(1,), name=\"protocol_type\", dtype=\"string\")\n",
    "service = keras.Input(shape=(1,), name=\"service\", dtype=\"string\")\n",
    "src_bytes = keras.Input(shape=(1,), name=\"src_bytes\", dtype=\"string\")\n",
    "dst_bytes = keras.Input(shape=(1,), name=\"dst_bytes\")\n",
    "flag = keras.Input(shape=(1,), name=\"flag\")\n",
    "land = keras.Input(shape=(1,), name=\"land\")\n",
    "wrong_fragment = keras.Input(shape=(1,), name=\"wrong_fragment\")\n",
    "urgent = keras.Input(shape=(1,), name=\"urgent\")\n",
    "hot = keras.Input(shape=(1,), name=\"hot\")\n",
    "num_failed_logins = keras.Input(shape=(1,), name=\"num_failed_logins\")\n",
    "logged_in = keras.Input(shape=(1,), name=\"logged_in\")\n",
    "num_compromised = keras.Input(shape=(1,), name=\"num_compromised\")\n",
    "root_shell = keras.Input(shape=(1,), name=\"root_shell\")\n",
    "su_attempted = keras.Input(shape=(1,), name=\"su_attempted\")\n",
    "num_root = keras.Input(shape=(1,), name=\"num_root\")\n",
    "num_file_creations = keras.Input(shape=(1,), name=\"num_file_creations\")\n",
    "num_shells = keras.Input(shape=(1,), name=\"num_shells\")\n",
    "num_access_files = keras.Input(shape=(1,), name=\"num_access_files\")\n",
    "num_outbound_cmds = keras.Input(shape=(1,), name=\"num_outbound_cmds\")\n",
    "is_hot_login = keras.Input(shape=(1,), name=\"is_hot_login\")\n",
    "is_guest_login = keras.Input(shape=(1,), name=\"is_guest_login\")\n",
    "count = keras.Input(shape=(1,), name=\"count\")\n",
    "serror_rate = keras.Input(shape=(1,), name=\"serror_rate\")\n",
    "rerror_rate = keras.Input(shape=(1,), name=\"rerror_rate\")\n",
    "same_srv_rate = keras.Input(shape=(1,), name=\"same_srv_rate\")\n",
    "diff_srv_rate = keras.Input(shape=(1,), name=\"diff_srv_rate\")\n",
    "srv_count = keras.Input(shape=(1,), name=\"srv_count\")\n",
    "srv_serror_rate = keras.Input(shape=(1,), name=\"srv_serror_rate\")\n",
    "srv_rerror_rate = keras.Input(shape=(1,), name=\"srv_rerror_rate\")\n",
    "srv_diff_host_rate = keras.Input(shape=(1,), name=\"srv_diff_host_rate\")\n",
    "# unknown_data1 = keras.Input(shape=(1,), name=\"unknown_data1\")\n",
    "# unknown_data2 = keras.Input(shape=(1,), name=\"unknown_data2\")\n",
    "# unknown_data3 = keras.Input(shape=(1,), name=\"unknown_data3\")\n",
    "# unknown_data4 = keras.Input(shape=(1,), name=\"unknown_data4\")\n",
    "# unknown_data5 = keras.Input(shape=(1,), name=\"unknown_data5\")\n",
    "# unknown_data6 = keras.Input(shape=(1,), name=\"unknown_data6\")\n",
    "# unknown_data7 = keras.Input(shape=(1,), name=\"unknown_data7\")\n",
    "# unknown_data8 = keras.Input(shape=(1,), name=\"unknown_data8\")\n",
    "# unknown_data9 = keras.Input(shape=(1,), name=\"unknown_data9\")\n",
    "# unknown_data10 = keras.Input(shape=(1,), name=\"unknown_data10\")\n",
    "\n",
    "\n",
    "all_inputs = [duration,\n",
    "              protocol_type, \n",
    "              service, \n",
    "              src_bytes, \n",
    "              dst_bytes, \n",
    "              flag, \n",
    "              land, \n",
    "              wrong_fragment,\n",
    "              urgent,\n",
    "              hot, \n",
    "              num_failed_logins, \n",
    "              logged_in, \n",
    "              num_compromised, \n",
    "              root_shell, \n",
    "              su_attempted, \n",
    "              num_root, \n",
    "              num_file_creations,\n",
    "              num_shells, \n",
    "              num_access_files, \n",
    "              num_outbound_cmds, \n",
    "              is_hot_login, \n",
    "              is_guest_login,\n",
    "              count, \n",
    "              serror_rate, \n",
    "              rerror_rate, \n",
    "              same_srv_rate, \n",
    "              diff_srv_rate, \n",
    "              srv_count, \n",
    "              srv_serror_rate, \n",
    "              srv_rerror_rate, \n",
    "              srv_diff_host_rate,\n",
    "#               unknown_data1,\n",
    "#               unknown_data2,\n",
    "#               unknown_data3,\n",
    "#               unknown_data4,\n",
    "#               unknown_data5,\n",
    "#               unknown_data6,\n",
    "#               unknown_data7,\n",
    "#               unknown_data8,\n",
    "#               unknown_data9,\n",
    "#               unknown_data10\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "weighted-duncan",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_encoded = encode_numerical_feature(duration, \"duration\", train_ds)\n",
    "\n",
    "protocol_type_encoded = encode_string_categorical_feature(protocol_type,\"protocol_type\", train_ds)\n",
    "service_encoded = encode_string_categorical_feature(service, \"service\", train_ds)\n",
    "src_bytes_encoded = encode_string_categorical_feature(src_bytes,\"src_bytes\", train_ds)\n",
    "\n",
    "\n",
    "dst_bytes_encoded = encode_numerical_feature(dst_bytes,\"dst_bytes\", train_ds)\n",
    "flag_encoded = encode_numerical_feature(flag, \"flag\", train_ds)\n",
    "land_encoded = encode_numerical_feature(land, \"land\", train_ds)\n",
    "wrong_fragment_encoded = encode_numerical_feature(wrong_fragment, \"wrong_fragment\", train_ds)\n",
    "urgent_encoded = encode_numerical_feature(urgent, \"urgent\", train_ds)\n",
    "hot_encoded = encode_numerical_feature(hot, \"hot\", train_ds)\n",
    "num_failed_logins_encoded = encode_numerical_feature(num_failed_logins, \"num_failed_logins\", train_ds)\n",
    "logged_in_encoded = encode_numerical_feature(logged_in, \"logged_in\", train_ds)\n",
    "num_compromised_encoded = encode_numerical_feature(num_compromised, \"num_compromised\", train_ds)\n",
    "root_shell_encoded = encode_numerical_feature(root_shell, \"root_shell\", train_ds)\n",
    "su_attempted_encoded = encode_numerical_feature(su_attempted, \"su_attempted\", train_ds)\n",
    "num_root_encoded = encode_numerical_feature(num_root, \"num_root\", train_ds)\n",
    "num_file_creations_encoded = encode_numerical_feature(num_file_creations, \"num_file_creations\", train_ds)\n",
    "num_shells_encoded = encode_numerical_feature(num_shells, \"num_shells\", train_ds)\n",
    "num_access_files_encoded = encode_numerical_feature(num_access_files, \"num_access_files\", train_ds)\n",
    "num_outbound_cmds_encoded = encode_numerical_feature(num_outbound_cmds, \"num_outbound_cmds\", train_ds)\n",
    "is_hot_login_encoded = encode_numerical_feature(is_hot_login, \"is_hot_login\", train_ds)\n",
    "is_guest_login_encoded = encode_numerical_feature(is_guest_login, \"is_guest_login\", train_ds)\n",
    "count_encoded = encode_numerical_feature(count, \"count\", train_ds)\n",
    "serror_rate_encoded = encode_numerical_feature(serror_rate, \"serror_rate\", train_ds)\n",
    "rerror_rate_encoded = encode_numerical_feature(rerror_rate, \"rerror_rate\", train_ds)\n",
    "same_srv_rate_encoded = encode_numerical_feature(same_srv_rate, \"same_srv_rate\", train_ds)\n",
    "diff_srv_rate_encoded = encode_numerical_feature(diff_srv_rate, \"diff_srv_rate\", train_ds)\n",
    "srv_count_encoded = encode_numerical_feature(srv_count, \"srv_count\", train_ds)\n",
    "srv_serror_rate_encoded = encode_numerical_feature(srv_serror_rate, \"srv_serror_rate\", train_ds)\n",
    "srv_rerror_rate_encoded = encode_numerical_feature(srv_rerror_rate, \"srv_rerror_rate\", train_ds)\n",
    "srv_diff_host_rate_encoded = encode_numerical_feature(srv_diff_host_rate, \"srv_diff_host_rate\", train_ds)\n",
    "# unknown_data1_encoded = encode_numerical_feature(unknown_data1, \"unknown_data1\", train_ds)\n",
    "# unknown_data2_encoded = encode_numerical_feature(unknown_data2, \"unknown_data2\", train_ds)\n",
    "# unknown_data3_encoded = encode_numerical_feature(unknown_data3, \"unknown_data3\", train_ds)\n",
    "# unknown_data4_encoded = encode_numerical_feature(unknown_data4, \"unknown_data4\", train_ds)\n",
    "# unknown_data5_encoded = encode_numerical_feature(unknown_data5, \"unknown_data5\", train_ds)\n",
    "# unknown_data6_encoded = encode_numerical_feature(unknown_data6, \"unknown_data6\", train_ds)\n",
    "# unknown_data7_encoded = encode_numerical_feature(unknown_data7, \"unknown_data7\", train_ds)\n",
    "# unknown_data8_encoded = encode_numerical_feature(unknown_data8, \"unknown_data8\", train_ds)\n",
    "# unknown_data9_encoded = encode_numerical_feature(unknown_data9, \"unknown_data9\", train_ds)\n",
    "# unknown_data10_encoded = encode_numerical_feature(unknown_data10, \"unknown_data10\", train_ds)\n",
    "\n",
    "\n",
    "\n",
    "all_features = layers.concatenate(\n",
    "    [\n",
    "        duration_encoded,\n",
    "        protocol_type_encoded, \n",
    "        service_encoded, \n",
    "        src_bytes_encoded, \n",
    "        dst_bytes_encoded,\n",
    "        flag_encoded, \n",
    "        land_encoded, \n",
    "        wrong_fragment_encoded,\n",
    "        urgent_encoded,\n",
    "        hot_encoded, \n",
    "        num_failed_logins_encoded, \n",
    "        logged_in_encoded, \n",
    "        num_compromised_encoded, \n",
    "        root_shell_encoded, \n",
    "        su_attempted_encoded, \n",
    "        num_root_encoded, \n",
    "        num_file_creations_encoded,\n",
    "        num_shells_encoded, \n",
    "        num_access_files_encoded, \n",
    "        num_outbound_cmds_encoded, \n",
    "        is_hot_login_encoded, \n",
    "        is_guest_login_encoded,\n",
    "        count_encoded, \n",
    "        serror_rate_encoded, \n",
    "        rerror_rate_encoded, \n",
    "        same_srv_rate_encoded, \n",
    "        diff_srv_rate_encoded, \n",
    "        srv_count_encoded, \n",
    "        srv_serror_rate_encoded, \n",
    "        srv_rerror_rate_encoded, \n",
    "        srv_diff_host_rate_encoded,\n",
    "#         unknown_data1_encoded,\n",
    "#         unknown_data2_encoded,\n",
    "#         unknown_data3_encoded,\n",
    "#         unknown_data4_encoded,\n",
    "#         unknown_data5_encoded,\n",
    "#         unknown_data6_encoded,\n",
    "#         unknown_data7_encoded,\n",
    "#         unknown_data8_encoded,\n",
    "#         unknown_data9_encoded,\n",
    "#         unknown_data10_encoded,\n",
    "\n",
    "    ]\n",
    ")\n",
    "                                              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "altered-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\zzzz_projects\\kasun nn web design\\env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:595: UserWarning: Input dict contained keys ['unknown_data1', 'unknown_data2', 'unknown_data3', 'unknown_data4', 'unknown_data5', 'unknown_data6', 'unknown_data7', 'unknown_data8', 'unknown_data9', 'unknown_data10'] which did not match any model input. They will be ignored by the model.\n",
      "  [n for n in tensors.keys() if n not in ref_input_names])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12351/12351 [==============================] - 82s 6ms/step - loss: 0.1512 - accuracy: 0.9379 - val_loss: 0.0131 - val_accuracy: 0.9958\n",
      "Epoch 2/5\n",
      "12351/12351 [==============================] - 83s 6ms/step - loss: 0.0761 - accuracy: 0.9689 - val_loss: 0.0152 - val_accuracy: 0.9962\n",
      "Epoch 3/5\n",
      "12351/12351 [==============================] - 83s 6ms/step - loss: 0.0759 - accuracy: 0.9686 - val_loss: 0.0129 - val_accuracy: 0.9965\n",
      "Epoch 4/5\n",
      "12351/12351 [==============================] - 83s 6ms/step - loss: 0.0766 - accuracy: 0.9691 - val_loss: 0.0125 - val_accuracy: 0.9963\n",
      "Epoch 5/5\n",
      "12351/12351 [==============================] - 87s 6ms/step - loss: 0.0740 - accuracy: 0.9699 - val_loss: 0.0114 - val_accuracy: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2093c6ddd68>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = layers.Dense(7, activation=\"relu\")(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x1 = layers.Dense(32, activation=\"relu\")(x)\n",
    "x1 = layers.Dropout(0.5)(x1)\n",
    "\n",
    "output = layers.Dense(1, activation=\"sigmoid\")(x1)\n",
    "\n",
    "model = keras.Model(all_inputs, output)\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_ds, epochs=5, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-lemon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "coordinate-investigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49146256]]\n",
      "This particular patient had a 49.1 percent probability the probability of having a malware\n"
     ]
    }
   ],
   "source": [
    "sample = {\n",
    "    \"duration\"           : 10,\n",
    "    \"protocol_type\"      : 'icmp', \n",
    "    \"service\"            : 'ecr_i', \n",
    "    \"src_bytes\"          : 'SF', \n",
    "    \"dst_bytes\"          : 11, \n",
    "    \"flag\"               : 0, \n",
    "    \"land\"               : 0, \n",
    "    'wrong_fragment'     : 0,\n",
    "    'urgent'             : 0,\n",
    "    'hot'                : 0,\n",
    "    'num_failed_logins'  : 0,\n",
    "    'logged_in'          : 0,\n",
    "    'num_compromised'    : 0,\n",
    "    'root_shell'         : 0,\n",
    "    'su_attempted'       : 0,    \n",
    "    'num_root'           : 0,\n",
    "    'num_file_creations' : 1,\n",
    "    'num_shells'         : 0,\n",
    "    'num_access_files'   : 0,\n",
    "    'num_outbound_cmds'  : 0,\n",
    "    'is_hot_login'       : 0,\n",
    "    'is_guest_login'     : 0,\n",
    "    'count'              : 1,\n",
    "    'serror_rate'        : 0,\n",
    "    'rerror_rate'        : 0,\n",
    "    'same_srv_rate'      : 0,\n",
    "    'diff_srv_rate'      : 0,\n",
    "    'srv_count'          : 0,\n",
    "    'srv_serror_rate'    : 0,\n",
    "    'srv_rerror_rate'    : 0,\n",
    "    'srv_diff_host_rate' : 1,\n",
    "#     'unknown_data1'      : 0,\n",
    "#     'unknown_data2'      : 0,\n",
    "#     'unknown_data3'      : 0,\n",
    "#     'unknown_data4'      : 0,\n",
    "#     'unknown_data5'      : 0,\n",
    "#     'unknown_data6'      : 0,\n",
    "#     'unknown_data7'      : 0,\n",
    "#     'unknown_data8'      : 0,\n",
    "#     'unknown_data9'      : 0,\n",
    "#     'unknown_data10'     : 0,\n",
    "#     'results'            : 0,\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "predictions = model.predict(input_dict)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "print(\n",
    "    \"This particular patient had a %.1f percent probability \"\n",
    "    \"the probability of having a malware\" % (100 * predictions[0][0],)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-attachment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
